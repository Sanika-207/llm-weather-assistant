# llm-weather-assistant
LLM-powered weather assistant using React, FastAPI, LangChain, and OpenRouter

# ğŸŒ¤ï¸ LLM-Powered Weather Assistant

A minimal full-stack web application that demonstrates how a Large Language Model (LLM) can dynamically use external tools to answer real-world queries.  
The application allows users to ask weather-related questions in natural language and receive real-time responses.

---

## ğŸš€ Project Overview

This project consists of a React-based frontend and a FastAPI-based backend.  
The backend integrates **LangChain** with an **LLM via OpenRouter**, enabling the system to understand user intent and decide when to call a weather tool.

Instead of using hard-coded rules, the LLM interprets natural language queries and invokes the appropriate tool to fetch live weather data.

---

## ğŸ§  Key Features

- ğŸŒ React frontend with a clean and simple UI  
- âš¡ FastAPI backend for handling API requests  
- ğŸ¤– LLM integration using LangChain + OpenRouter  
- ğŸ› ï¸ Tool-based reasoning to fetch real-time weather data  
- ğŸ—£ï¸ Supports natural language queries like:
  - *â€œWhatâ€™s the weather of Pune?â€*
  - *â€œWeather of Mumbai todayâ€*
  - *â€œIs it hot in Delhi?â€*
- âŒ Graceful handling of invalid city names and API failures  

---

## ğŸ”„ Application Flow

User
â†“
React Frontend
â†“
FastAPI Backend
â†“
LangChain Agent
â†“
LLM (via OpenRouter)
â†“
Weather Tool (API)
â†“
Formatted Natural Language Response


---

## ğŸ› ï¸ Technology Stack

### Frontend
- React (Vite)
- JavaScript
- HTML & CSS

### Backend
- FastAPI
- Python

### AI & Tools
- LangChain
- OpenRouter (LLM provider)
- OpenWeather API

---

## ğŸ§© Why Use an LLM?

- Handles natural language variations instead of keyword matching  
- Dynamically decides when a tool is required  
- Clean separation between reasoning and business logic  
- Easily extensible to additional tools (e.g., air quality, forecast)

---

## âš ï¸ Error Handling

- Invalid city names are handled gracefully  
- External API failures do not crash the application  
- User-friendly error messages are returned  

---

## ğŸ“‚ Project Structure

llm-weather-assistant/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ agent.py
â”‚ â”œâ”€â”€ weather_tool.py
â”‚ â””â”€â”€ .env (ignored)
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ package.json
â”‚ â””â”€â”€ node_modules (ignored)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## â–¶ï¸ How to Run the Project

### Backend
```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload

Frontend-
cd frontend
npm install
npm run dev

Open the application at:
http://localhost:5173



ğŸŒ± Future Enhancements

Add more tools (air quality, humidity, forecast)

Improve UI styling and animations

Deploy the application to cloud platforms

ğŸ“ Summary

This project demonstrates how LLMs can be used beyond simple chat applications by enabling tool-based reasoning for real-world data access.
The focus is on clean architecture, reasoning transparency, and extensibility.